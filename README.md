# MFG_RL: Reinforcement Learning for Mean Field Games

A comprehensive Python library for solving Mean Field Games (MFGs) using Reinforcement Learning approaches. This repository provides a unified framework for implementing and comparing different RL algorithms in the context of mean field games.

## ğŸ“‹ Overview

Mean Field Games (MFGs) are a mathematical framework for modeling large-scale multi-agent systems where individual agents interact through a mean field (population distribution) rather than direct pairwise interactions. This library bridges the gap between the traditional PDE-based approaches and modern reinforcement learning techniques.

### Key Features

- ğŸš€ **Multiple RL Algorithms**: Implementations of DQN, Policy Gradient, and Actor-Critic methods adapted for MFGs
- ğŸŒ **Diverse Environments**: Linear-quadratic games, crowd motion models, and extensible base classes
- ğŸ§  **Neural Network Architectures**: Specialized networks for handling mean field interactions
- ğŸ“Š **Comprehensive Logging**: Integration with TensorBoard and Weights & Biases
- âš¡ **High Performance**: Optimized implementations using PyTorch/JAX
- ğŸ“š **Educational**: Well-documented examples and tutorials

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- PyTorch 1.9+ or TensorFlow 2.6+

### Install from source

```bash
git clone https://github.com/derrring/MFG_RL.git
cd MFG_RL
pip install -e .
```

### Development installation

```bash
git clone https://github.com/derrring/MFG_RL.git
cd MFG_RL
pip install -e ".[dev]"
```

## ğŸš€ Quick Start

Here's a simple example to get you started:

```python
from mfg_rl.environments import LinearQuadraticMFG
from mfg_rl.algorithms import MeanFieldDQN
from mfg_rl.utils import Config

# Configure the experiment
config = Config({
    'state_dim': 2,
    'action_dim': 1,
    'population_size': 100,
    'n_episodes': 1000
})

# Initialize environment and algorithm
env = LinearQuadraticMFG(config)
algorithm = MeanFieldDQN(config, env)

# Train the model
for episode in range(config.n_episodes):
    reward = algorithm.train_episode()
    if episode % 100 == 0:
        print(f"Episode {episode}, Reward: {reward:.3f}")
```

## ğŸ“ Repository Structure

```
MFG_RL/
â”œâ”€â”€ src/mfg_rl/              # Main package
â”‚   â”œâ”€â”€ agents/              # RL agent implementations
â”‚   â”œâ”€â”€ algorithms/          # MFG-specific RL algorithms
â”‚   â”œâ”€â”€ environments/        # Mean field game environments
â”‚   â”œâ”€â”€ networks/           # Neural network architectures
â”‚   â””â”€â”€ utils/              # Utilities and helper functions
â”œâ”€â”€ examples/               # Example scripts and tutorials
â”‚   â”œâ”€â”€ basic/             # Basic usage examples
â”‚   â””â”€â”€ advanced/          # Advanced implementations
â”œâ”€â”€ notebooks/             # Jupyter notebooks for analysis
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ data/                 # Data storage
â”‚   â”œâ”€â”€ raw/             # Raw experimental data
â”‚   â””â”€â”€ processed/       # Processed results
â”œâ”€â”€ results/             # Training outputs
â”‚   â”œâ”€â”€ models/          # Saved model checkpoints
â”‚   â”œâ”€â”€ logs/           # Training logs
â”‚   â””â”€â”€ plots/          # Visualization outputs
â””â”€â”€ docs/               # Documentation
```

## ğŸ¤– Supported Algorithms

### Deep Q-Learning (DQN) for MFGs
- **Mean Field DQN**: Adapts DQN to handle mean field interactions
- **Double DQN**: Reduces overestimation bias in MFG settings
- **Dueling DQN**: Separates state value and advantage estimation

### Policy Gradient Methods
- **Mean Field Policy Gradient**: Direct policy optimization with mean field coupling
- **Actor-Critic**: Combined value function approximation and policy optimization
- **Proximal Policy Optimization (PPO)**: Stable policy updates for MFGs

### Advanced Methods
- **Multi-Agent Actor-Critic**: Centralized training, decentralized execution
- **Mean Field Multi-Agent RL**: Population-based learning approaches

## ğŸ® Environments

### Linear Quadratic MFG
Classic benchmark problem with quadratic costs and linear dynamics.

```python
from mfg_rl.environments import LinearQuadraticMFG
env = LinearQuadraticMFG(config)
```

### Crowd Motion
Pedestrian dynamics and crowd flow simulation.

```python
from mfg_rl.environments import CrowdMotion
env = CrowdMotion(config)
```

### Custom Environments
Extend the `BaseEnvironment` class to create your own MFG environments.

## ğŸ“– Theoretical Background

Mean Field Games provide a mathematical framework for:

1. **Large Population Games**: Modeling systems with many interacting agents
2. **Nash Equilibrium**: Finding optimal strategies in competitive settings  
3. **Mean Field Coupling**: Agents interact through population statistics
4. **PDE Connections**: Links to Hamilton-Jacobi-Bellman and Fokker-Planck equations

The RL approach approximates the traditional PDE-based solution methods using:
- Function approximation via neural networks
- Sampling-based learning instead of solving PDEs directly
- Scalable algorithms for high-dimensional state/action spaces

## âš–ï¸ Comparison with PDE Methods

| Aspect | RL Approach (MFG_RL) | PDE Approach (MFG_PDE) |
|--------|---------------------|------------------------|
| **Scalability** | High-dimensional spaces | Limited by curse of dimensionality |
| **Flexibility** | Easy to modify rewards/dynamics | Requires analytical derivatives |
| **Convergence** | Approximate, empirical | Theoretical guarantees |
| **Implementation** | Straightforward | Complex numerical schemes |
| **Interpretability** | Limited | High theoretical insight |

## ğŸ”¬ Running Experiments

### Basic Linear Quadratic Example
```bash
python examples/basic/linear_quadratic_example.py
```

### Advanced Multi-Agent Scenarios
```bash
python examples/advanced/crowd_motion_experiment.py --config config/crowd_motion.yaml
```

### Custom Configuration
```bash
python examples/basic/linear_quadratic_example.py --config custom_config.yaml
```

## ğŸ“ˆ Monitoring and Visualization

### TensorBoard Integration
```bash
tensorboard --logdir results/logs
```

### Weights & Biases
Set `wandb: true` in your configuration file to enable automatic experiment tracking.

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/mfg_rl --cov-report=html

# Run specific test categories
pytest tests/test_algorithms.py
```

## ğŸ“– Documentation

### API Documentation
```bash
cd docs/
make html
```

### Jupyter Notebooks
Explore the `notebooks/` directory for interactive tutorials and analysis examples.

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Workflow
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests and ensure they pass
5. Submit a pull request

### Code Style
We use Black for code formatting and flake8 for linting:

```bash
black src/ tests/
flake8 src/ tests/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Citation

If you use this library in your research, please cite:

```bibtex
@software{mfg_rl2024,
  title={MFG_RL: Reinforcement Learning for Mean Field Games},
  author={derrring},
  year={2024},
  url={https://github.com/derrring/MFG_RL}
}
```

## ğŸ”— Related Work

- **MFG_PDE**: PDE-based approaches for Mean Field Games
- **Multi-Agent RL**: Traditional multi-agent reinforcement learning
- **Game Theory**: Classical game-theoretic solution concepts

## ğŸ™ Acknowledgments

- Mean Field Game theory pioneers: Lasry-Lions and Huang-Caines-MalhamÃ©
- Open source RL community (Stable Baselines3, Ray RLlib)
- PyTorch and JAX development teams

---

**Note**: This is an active research project. APIs may change as the library evolves.