# Default configuration for MFG_RL experiments

# Environment settings
environment:
  type: "LinearQuadraticMFG"  # LinearQuadraticMFG, CrowdMotion
  state_dim: 2
  action_dim: 1
  population_size: 100
  episode_length: 50
  dt: 0.01  # time step

# Algorithm settings  
algorithm:
  type: "MeanFieldDQN"  # MeanFieldDQN, MeanFieldPolicyGradient, MeanFieldActorCritic
  learning_rate: 0.001
  batch_size: 32
  memory_size: 10000
  target_update_frequency: 100
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  
# Network architecture
network:
  hidden_layers: [64, 64]
  activation: "relu"  # relu, tanh, sigmoid
  dropout: 0.0

# Training settings
training:
  n_episodes: 1000
  eval_frequency: 100
  save_frequency: 500
  max_episode_steps: 200

# Logging and visualization
logging:
  log_dir: "results/logs"
  tensorboard: true
  wandb: false
  wandb_project: "mfg_rl"
  
# Data settings
data:
  save_trajectories: true
  trajectory_dir: "data/processed/trajectories"
  
# Reproducibility
seed: 42